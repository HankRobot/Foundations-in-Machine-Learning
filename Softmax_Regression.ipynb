{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HankRobot/Foundations-in-Machine-Learning/blob/main/Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFSihVyJGDQ"
      },
      "source": [
        "# Softmax Regression\n",
        "https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html#:~:text=Softmax%20Regression%20(synonyms%3A%20Multinomial%20Logistic,the%20classes%20are%20mutually%20exclusive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scq-UQdtI56o"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0QFfJhkJcRb"
      },
      "source": [
        "### Linear function for score \n",
        "For data point x, the score for class  k  is  $a_k=w_{k0}+∑_jw_{kj}x_j=w_0+w^⊤_kx$ . The predicted probability is\n",
        "$y_k=exp(a_k)/∑_iexp(a_i)$.\n",
        " \n",
        "Below we give different implementations of the probability. First, via a for loop, so you can see all the components appearing explicitly. Next, in softmax_prob1 this is presented in vectorised form. Note that exponentiation can cause over/under flow problems. There is a fix that I have introduced that relies on\n",
        "$y_k=exp(a_k−A)/∑_iexp(a_i−A)$\n",
        " \n",
        "for any  A ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uxdUhopI6TE"
      },
      "source": [
        "def softmax_prob_forloop(W, b, inputs): # output is datalen-by-C (NumPy, no JAX here)\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    dim, datalen = np.shape(inputs) # how many dimensions, points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    score = np.zeros((c, datalen))\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = b[ci]\n",
        "            for dk in range(dim):\n",
        "                score[ci, lj] += W[ci, dk]*inputs[dk, lj]\n",
        "    maxes = np.zeros(datalen)\n",
        "    for lj in range(datalen):\n",
        "        maxes[lj] = np.max(score[:, lj])\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = score[ci, lj] - maxes[lj]\n",
        "    # subtract off the largest score from the bias of each class \n",
        "    # This is for stability to underflow/overflow when exponentiating\n",
        "    expscore = np.exp(score)\n",
        "    norm_factor = np.diag(1/np.sum(expscore, axis=0))\n",
        "    return np.dot(expscore, norm_factor).T  \n",
        "\n",
        "\n",
        "# below we convert the same steps into vector form, hence no for loops\n",
        "\n",
        "def softmax_prob1(W, b, inputs):  # output is datalen-by-C\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    linear_part = jnp.dot(W, inputs) # (C-by-dim)*(dim-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(linear_part, axis=0) # largest of the class scores for each data point\n",
        "    bias_offset = jnp.dot(jnp.diag(b),jnp.ones((c, datalen))) # (C-by-C)*(C-by-L)\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(linear_part + bias_offset - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8847DKq3Jk9L"
      },
      "source": [
        "In what follows, the trick of setting the zeroth feature to be 1 is used to absorb the constant  $w_0$  into the dot product. Redefine the input data to be\n",
        "$x=(x1,…,xp)⟶x=(1,x1,…,xp)$.\n",
        " \n",
        "Correspondingly redefining the weight vectors to be  $w=(w0,w1,…,wp)$ , we have:\n",
        "$w_{k0}+w^⊤_kx⟶w^⊤_kx$.\n",
        " \n",
        "Thus the softmax_prob below has all the weights packaged into a matrix W as in the lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLRkL-6JCUd"
      },
      "source": [
        "def softmax_prob(W, inputs):  \n",
        "    # output is datalen-by-C\n",
        "    # inputs is (dim)-by-datalen\n",
        "    # W is C-by-(dim+1)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(W) # number of classes, C, each class has a bias\n",
        "    inputs = jnp.concatenate((jnp.ones((1,datalen)), inputs), axis=0)\n",
        "    # create inputs (dim+1)-by-datalen \n",
        "    score = jnp.dot(W,inputs) \n",
        "    # (C-by-(1+dim))*((1+dim)-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(score, axis=0) # largest of the class scores for each data point\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(score  - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZmXBwhtJoYp"
      },
      "source": [
        "def softmax_xentropy(Wb, inputs, targets, num_classes):\n",
        "    epsilon = 1e-8\n",
        "    ys = get_one_hot(targets, num_classes)\n",
        "    logprobs = -jnp.log(softmax_prob(Wb, inputs)+epsilon)\n",
        "    return jnp.mean(ys*logprobs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tU86xW-JpYG"
      },
      "source": [
        "def get_one_hot(targets, num_classes):\n",
        "    res = jnp.eye(num_classes)[jnp.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[num_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyU2kr7JqYJ",
        "outputId": "131b781c-fbdd-4163-8c18-430bd8b32cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "Wb = jnp.array([[-3., 1.3, 2.0, -1.0], [-6., -2., -3., 1.5], [1., 2.0, 2.0, 2.5], [3., 4.0, 4.0, -2.5]])\n",
        "# Build a toy dataset: 6 3-dim points with C=4  targets dim-by-datalen\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [3.82, -6.11, 3.15],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39],\n",
        "                   [0.14, -0.43, -1.69]]).T # transpose to make it a dim-by-datalen array\n",
        "targets = jnp.array([0, 1, 3, 2, 1, 2])\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.52  3.82  0.88  0.52  0.74  0.14]\n",
            " [ 1.12 -6.11 -1.08  0.06 -2.49 -0.43]\n",
            " [ 0.77  3.15  0.15 -1.3   1.39 -1.69]]\n",
            "[0 1 3 2 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xoxpsuGJsBP",
        "outputId": "709d4d7c-07a0-4290-801f-2b6025f218df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.20820075 -1.0580498  -0.2937458  -0.44117242]\n",
            " [ 0.2366985  -0.03426379 -1.002556    1.1560112 ]\n",
            " [-0.538138   -0.48968914  0.2493904  -1.4128864 ]\n",
            " [ 1.8543109   0.22756508  0.4975155  -2.0896842 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY8vxPe3JvVI"
      },
      "source": [
        "## Automatic Differentiation used here\n",
        "Here, we will not explicitly define what the exact form of the gradient of the cross entropy loss function is. Recall, for linear regression, we computed the gradient and used it to reduce the loss. In this next code block, we will invoke\n",
        "\n",
        "grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "\n",
        "where the (0) is shorthand for argnums=0 which indicates that we take the gradient with respect to the first (using python's indexing convention of starting from 0) of the arguments of softmax_entropy. How this is done will be explored in another lab sheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97S7opPJtdL"
      },
      "source": [
        "def grad_descent(Wb, inputs, targets, num_classes,  lrate, nsteps):\n",
        "    W1 = Wb\n",
        "    Whist = [W1]\n",
        "    losshist = [softmax_xentropy(W1,inputs, targets, num_classes )]\n",
        "    eta = lrate # learning rate\n",
        "    for i in range(nsteps):        \n",
        "        gWb = grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "        W1 = W1 - eta*gWb\n",
        "        if (i%5 ==0):\n",
        "            Whist.append(W1)\n",
        "            losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))\n",
        "    Whist.append(W1)\n",
        "    losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))    \n",
        "    return W1, Whist, losshist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyMN5NFiJz1y"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.75, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4ytRwaJ7hW"
      },
      "source": [
        "### Loss history\n",
        "Now that we have the initial weights Winit, the history of weights and the history of losses, we can see how the loss function reduces as a function of iteration step. You should experiment with different learning rates and iteration steps, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtwDXHjOJ8dm",
        "outputId": "867cb6b1-07f9-4b10-c16e-b294c9aff6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0de6b9e898>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ntpzf30KFv8"
      },
      "source": [
        "Compare the predictions with the targets. First, we see what the randomly initialised weights produced as the predicted probabilities. Then we note the final (at the point that we stopped the iterations) prediction and compare that with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MYK_0INJ3ef",
        "outputId": "20462504-4359-45a5-8ced-b01ea3daf355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "print('From:\\n',np.around(softmax_prob(Winit, inputs),3))\n",
        "print('To:\\n',np.around(softmax_prob(W2, inputs),3))\n",
        "print('Target:\\n',get_one_hot(targets, 4))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From:\n",
            " [[0.09  0.243 0.05  0.618]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.073 0.507 0.028 0.392]\n",
            " [0.011 0.002 0.025 0.962]\n",
            " [0.008 0.99  0.    0.002]\n",
            " [0.013 0.001 0.028 0.957]]\n",
            "To:\n",
            " [[0.951 0.009 0.006 0.034]\n",
            " [0.    0.996 0.    0.004]\n",
            " [0.025 0.13  0.043 0.802]\n",
            " [0.004 0.    0.949 0.047]\n",
            " [0.001 0.926 0.    0.074]\n",
            " [0.    0.    0.979 0.021]]\n",
            "Target:\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Bb0ZCEKL4_"
      },
      "source": [
        "# Your turn:\n",
        "Create your own input data and targets. You may choose them to be random. For instance, in numpy np.random.normal(mean, std_dev,(dim, datalen)) will create a set of datalen inputs of dimension dim, drawn from a normal distribution of a chosen mean and standard deviation. You must generate the Winit from jax.numpy in order to be able to use the gradient. Experiment with different learning rates, and see what you find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1b1Lx0kKMPk",
        "outputId": "0d34af09-62f6-4bfe-f443-780bb13c3105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.20820075 -1.0580498  -0.2937458  -0.44117242]\n",
            " [ 0.2366985  -0.03426379 -1.002556    1.1560112 ]\n",
            " [-0.538138   -0.48968914  0.2493904  -1.4128864 ]\n",
            " [ 1.8543109   0.22756508  0.4975155  -2.0896842 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Hww71UFmsM",
        "outputId": "36d73469-1c89-4e8c-dd3a-34c307d202c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "inputs = np.random.normal(1, 0.2, (6,3)).T \n",
        "targets = np.random.randint(4,size=6)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.03884827 1.22674478 1.15922851 1.04414937 0.75891913 0.85492658]\n",
            " [1.1037261  0.91104062 0.99841792 1.282987   0.54458355 0.82857137]\n",
            " [0.95715109 0.91626401 1.62206523 1.18242355 1.07221157 1.27152793]]\n",
            "[3 2 0 3 0 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI_5awlRMie0"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUaWbQwxECpt"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.01, 1000)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovKi4plaE3FZ",
        "outputId": "2c2b2505-5e76-4a04-c74f-9dab8325c80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0de68e27f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHtcY6eFbb2",
        "outputId": "6a4233a9-b3ca-4a52-e4b7-4d47dd659db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "print('From:\\n',np.around(softmax_prob(Winit, inputs),3))\n",
        "print('To:\\n',np.around(softmax_prob(W2, inputs),3))\n",
        "print('Target:\\n',get_one_hot(targets, 4))\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From:\n",
            " [[0.057 0.356 0.035 0.552]\n",
            " [0.047 0.386 0.03  0.537]\n",
            " [0.037 0.821 0.012 0.13 ]\n",
            " [0.058 0.46  0.032 0.45 ]\n",
            " [0.075 0.63  0.026 0.27 ]\n",
            " [0.065 0.675 0.023 0.238]]\n",
            "To:\n",
            " [[0.212 0.041 0.108 0.638]\n",
            " [0.193 0.05  0.104 0.653]\n",
            " [0.436 0.135 0.097 0.332]\n",
            " [0.267 0.045 0.115 0.573]\n",
            " [0.349 0.142 0.092 0.417]\n",
            " [0.38  0.118 0.098 0.403]]\n",
            "Target:\n",
            " [[0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8HkM-gNOHK"
      },
      "source": [
        "Conclusiom: Random weights with random weights easily results in bad accurcay, it would be best if the weights are initialized to be close to the targets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0gE_kHJIKN8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}