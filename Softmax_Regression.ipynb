{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HankRobot/Foundations-in-Machine-Learning/blob/main/Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFSihVyJGDQ"
      },
      "source": [
        "# Softmax Regression\n",
        "https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html#:~:text=Softmax%20Regression%20(synonyms%3A%20Multinomial%20Logistic,the%20classes%20are%20mutually%20exclusive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scq-UQdtI56o"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0QFfJhkJcRb"
      },
      "source": [
        "### Linear function for score \n",
        "For data point x, the score for class  k  is  $a_k=w_{k0}+∑_jw_{kj}x_j=w_0+w^⊤_kx$ . The predicted probability is\n",
        "$y_k=exp(a_k)/∑_iexp(a_i)$.\n",
        " \n",
        "Below we give different implementations of the probability. First, via a for loop, so you can see all the components appearing explicitly. Next, in softmax_prob1 this is presented in vectorised form. Note that exponentiation can cause over/under flow problems. There is a fix that I have introduced that relies on\n",
        "$y_k=exp(a_k−A)/∑_iexp(a_i−A)$\n",
        " \n",
        "for any  A ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uxdUhopI6TE"
      },
      "source": [
        "def softmax_prob_forloop(W, b, inputs): # output is datalen-by-C (NumPy, no JAX here)\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    dim, datalen = np.shape(inputs) # how many dimensions, points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    score = np.zeros((c, datalen))\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = b[ci]\n",
        "            for dk in range(dim):\n",
        "                score[ci, lj] += W[ci, dk]*inputs[dk, lj]\n",
        "    maxes = np.zeros(datalen)\n",
        "    for lj in range(datalen):\n",
        "        maxes[lj] = np.max(score[:, lj])\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = score[ci, lj] - maxes[lj]\n",
        "    # subtract off the largest score from the bias of each class \n",
        "    # This is for stability to underflow/overflow when exponentiating\n",
        "    expscore = np.exp(score)\n",
        "    norm_factor = np.diag(1/np.sum(expscore, axis=0))\n",
        "    return np.dot(expscore, norm_factor).T  \n",
        "\n",
        "\n",
        "# below we convert the same steps into vector form, hence no for loops\n",
        "\n",
        "def softmax_prob1(W, b, inputs):  # output is datalen-by-C\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    linear_part = jnp.dot(W, inputs) # (C-by-dim)*(dim-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(linear_part, axis=0) # largest of the class scores for each data point\n",
        "    bias_offset = jnp.dot(jnp.diag(b),jnp.ones((c, datalen))) # (C-by-C)*(C-by-L)\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(linear_part + bias_offset - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8847DKq3Jk9L"
      },
      "source": [
        "In what follows, the trick of setting the zeroth feature to be 1 is used to absorb the constant  $w_0$  into the dot product. Redefine the input data to be\n",
        "$x=(x1,…,xp)⟶x=(1,x1,…,xp)$.\n",
        " \n",
        "Correspondingly redefining the weight vectors to be  $w=(w0,w1,…,wp)$ , we have:\n",
        "$w_{k0}+w^⊤_kx⟶w^⊤_kx$.\n",
        " \n",
        "Thus the softmax_prob below has all the weights packaged into a matrix W as in the lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLRkL-6JCUd"
      },
      "source": [
        "def softmax_prob(W, inputs):  \n",
        "    # output is datalen-by-C\n",
        "    # inputs is (dim)-by-datalen\n",
        "    # W is C-by-(dim+1)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(W) # number of classes, C, each class has a bias\n",
        "    inputs = jnp.concatenate((jnp.ones((1,datalen)), inputs), axis=0)\n",
        "    # create inputs (dim+1)-by-datalen \n",
        "    score = jnp.dot(W,inputs) \n",
        "    # (C-by-(1+dim))*((1+dim)-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(score, axis=0) # largest of the class scores for each data point\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(score  - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZmXBwhtJoYp"
      },
      "source": [
        "def softmax_xentropy(Wb, inputs, targets, num_classes):\n",
        "    epsilon = 1e-8\n",
        "    ys = get_one_hot(targets, num_classes)\n",
        "    logprobs = -jnp.log(softmax_prob(Wb, inputs)+epsilon)\n",
        "    return jnp.mean(ys*logprobs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tU86xW-JpYG"
      },
      "source": [
        "def get_one_hot(targets, num_classes):\n",
        "    res = jnp.eye(num_classes)[jnp.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[num_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyU2kr7JqYJ",
        "outputId": "131b781c-fbdd-4163-8c18-430bd8b32cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "Wb = jnp.array([[-3., 1.3, 2.0, -1.0], [-6., -2., -3., 1.5], [1., 2.0, 2.0, 2.5], [3., 4.0, 4.0, -2.5]])\n",
        "# Build a toy dataset: 6 3-dim points with C=4  targets dim-by-datalen\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [3.82, -6.11, 3.15],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39],\n",
        "                   [0.14, -0.43, -1.69]]).T # transpose to make it a dim-by-datalen array\n",
        "targets = jnp.array([0, 1, 3, 2, 1, 2])\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.52  3.82  0.88  0.52  0.74  0.14]\n",
            " [ 1.12 -6.11 -1.08  0.06 -2.49 -0.43]\n",
            " [ 0.77  3.15  0.15 -1.3   1.39 -1.69]]\n",
            "[0 1 3 2 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xoxpsuGJsBP",
        "outputId": "709d4d7c-07a0-4290-801f-2b6025f218df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.20820075 -1.0580498  -0.2937458  -0.44117242]\n",
            " [ 0.2366985  -0.03426379 -1.002556    1.1560112 ]\n",
            " [-0.538138   -0.48968914  0.2493904  -1.4128864 ]\n",
            " [ 1.8543109   0.22756508  0.4975155  -2.0896842 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY8vxPe3JvVI"
      },
      "source": [
        "## Automatic Differentiation used here\n",
        "Here, we will not explicitly define what the exact form of the gradient of the cross entropy loss function is. Recall, for linear regression, we computed the gradient and used it to reduce the loss. In this next code block, we will invoke\n",
        "\n",
        "grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "\n",
        "where the (0) is shorthand for argnums=0 which indicates that we take the gradient with respect to the first (using python's indexing convention of starting from 0) of the arguments of softmax_entropy. How this is done will be explored in another lab sheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97S7opPJtdL"
      },
      "source": [
        "def grad_descent(Wb, inputs, targets, num_classes,  lrate, nsteps):\n",
        "    W1 = Wb\n",
        "    Whist = [W1]\n",
        "    losshist = [softmax_xentropy(W1,inputs, targets, num_classes )]\n",
        "    eta = lrate # learning rate\n",
        "    for i in range(nsteps):        \n",
        "        gWb = grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "        W1 = W1 - eta*gWb\n",
        "        if (i%5 ==0):\n",
        "            Whist.append(W1)\n",
        "            losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))\n",
        "    Whist.append(W1)\n",
        "    losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))    \n",
        "    return W1, Whist, losshist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyMN5NFiJz1y"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.75, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4ytRwaJ7hW"
      },
      "source": [
        "### Loss history\n",
        "Now that we have the initial weights Winit, the history of weights and the history of losses, we can see how the loss function reduces as a function of iteration step. You should experiment with different learning rates and iteration steps, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtwDXHjOJ8dm",
        "outputId": "867cb6b1-07f9-4b10-c16e-b294c9aff6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0de6b9e898>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ntpzf30KFv8"
      },
      "source": [
        "Compare the predictions with the targets. First, we see what the randomly initialised weights produced as the predicted probabilities. Then we note the final (at the point that we stopped the iterations) prediction and compare that with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MYK_0INJ3ef",
        "outputId": "20462504-4359-45a5-8ced-b01ea3daf355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "print('From:\\n',np.around(softmax_prob(Winit, inputs),3))\n",
        "print('To:\\n',np.around(softmax_prob(W2, inputs),3))\n",
        "print('Target:\\n',get_one_hot(targets, 4))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From:\n",
            " [[0.09  0.243 0.05  0.618]\n",
            " [0.    1.    0.    0.   ]\n",
            " [0.073 0.507 0.028 0.392]\n",
            " [0.011 0.002 0.025 0.962]\n",
            " [0.008 0.99  0.    0.002]\n",
            " [0.013 0.001 0.028 0.957]]\n",
            "To:\n",
            " [[0.951 0.009 0.006 0.034]\n",
            " [0.    0.996 0.    0.004]\n",
            " [0.025 0.13  0.043 0.802]\n",
            " [0.004 0.    0.949 0.047]\n",
            " [0.001 0.926 0.    0.074]\n",
            " [0.    0.    0.979 0.021]]\n",
            "Target:\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Bb0ZCEKL4_"
      },
      "source": [
        "# Your turn:\n",
        "Create your own input data and targets. You may choose them to be random. For instance, in numpy np.random.normal(mean, std_dev,(dim, datalen)) will create a set of datalen inputs of dimension dim, drawn from a normal distribution of a chosen mean and standard deviation. You must generate the Winit from jax.numpy in order to be able to use the gradient. Experiment with different learning rates, and see what you find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1b1Lx0kKMPk",
        "outputId": "0d34af09-62f6-4bfe-f443-780bb13c3105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.20820075 -1.0580498  -0.2937458  -0.44117242]\n",
            " [ 0.2366985  -0.03426379 -1.002556    1.1560112 ]\n",
            " [-0.538138   -0.48968914  0.2493904  -1.4128864 ]\n",
            " [ 1.8543109   0.22756508  0.4975155  -2.0896842 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Hww71UFmsM",
        "outputId": "36d73469-1c89-4e8c-dd3a-34c307d202c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "inputs = np.random.normal(1, 0.2, (6,3)).T \n",
        "targets = np.random.randint(4,size=6)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.03884827 1.22674478 1.15922851 1.04414937 0.75891913 0.85492658]\n",
            " [1.1037261  0.91104062 0.99841792 1.282987   0.54458355 0.82857137]\n",
            " [0.95715109 0.91626401 1.62206523 1.18242355 1.07221157 1.27152793]]\n",
            "[3 2 0 3 0 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUaWbQwxECpt"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.1, 200)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovKi4plaE3FZ",
        "outputId": "9d0e3a8d-29ce-4e5c-baf2-568d83761717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0de6a114a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CHtcY6eFbb2",
        "outputId": "8fe51393-3a87-49d1-8ede-d79318a7be97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "print('From:\\n',np.around(softmax_prob(Winit, inputs),3))\n",
        "print('To:\\n',np.around(softmax_prob(W2, inputs),3))\n",
        "print('Target:\\n',get_one_hot(targets, 4))\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From:\n",
            " [[0.057 0.356 0.035 0.552]\n",
            " [0.047 0.386 0.03  0.537]\n",
            " [0.037 0.821 0.012 0.13 ]\n",
            " [0.058 0.46  0.032 0.45 ]\n",
            " [0.075 0.63  0.026 0.27 ]\n",
            " [0.065 0.675 0.023 0.238]]\n",
            "To:\n",
            " [[0.206 0.019 0.146 0.629]\n",
            " [0.194 0.024 0.146 0.635]\n",
            " [0.476 0.057 0.137 0.33 ]\n",
            " [0.258 0.019 0.154 0.568]\n",
            " [0.385 0.079 0.12  0.417]\n",
            " [0.408 0.058 0.13  0.404]]\n",
            "Target:\n",
            " [[0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0gE_kHJIKN8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}