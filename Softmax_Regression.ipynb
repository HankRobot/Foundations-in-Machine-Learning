{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HankRobot/Foundations-in-Machine-Learning/blob/main/Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laFSihVyJGDQ"
      },
      "source": [
        "# Softmax Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scq-UQdtI56o"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0QFfJhkJcRb"
      },
      "source": [
        "### Linear function for score \n",
        "For data point x, the score for class  k  is  $a_k=w_{k0}+∑_jw_{kj}x_j=w_0+w^⊤_kx$ . The predicted probability is\n",
        "$y_k=exp(a_k)/∑_iexp(a_i)$.\n",
        " \n",
        "Below we give different implementations of the probability. First, via a for loop, so you can see all the components appearing explicitly. Next, in softmax_prob1 this is presented in vectorised form. Note that exponentiation can cause over/under flow problems. There is a fix that I have introduced that relies on\n",
        "$y_k=exp(a_k−A)/∑_iexp(a_i−A)$\n",
        " \n",
        "for any  A ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uxdUhopI6TE"
      },
      "source": [
        "def softmax_prob_forloop(W, b, inputs): # output is datalen-by-C (NumPy, no JAX here)\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    dim, datalen = np.shape(inputs) # how many dimensions, points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    score = np.zeros((c, datalen))\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = b[ci]\n",
        "            for dk in range(dim):\n",
        "                score[ci, lj] += W[ci, dk]*inputs[dk, lj]\n",
        "    maxes = np.zeros(datalen)\n",
        "    for lj in range(datalen):\n",
        "        maxes[lj] = np.max(score[:, lj])\n",
        "    for ci in range(c):\n",
        "        for lj in range(datalen):\n",
        "            score[ci, lj] = score[ci, lj] - maxes[lj]\n",
        "    # subtract off the largest score from the bias of each class \n",
        "    # This is for stability to underflow/overflow when exponentiating\n",
        "    expscore = np.exp(score)\n",
        "    norm_factor = np.diag(1/np.sum(expscore, axis=0))\n",
        "    return np.dot(expscore, norm_factor).T  \n",
        "\n",
        "\n",
        "# below we convert the same steps into vector form, hence no for loops\n",
        "\n",
        "def softmax_prob1(W, b, inputs):  # output is datalen-by-C\n",
        "    # inputs is dim-by-datalen\n",
        "    # b is C-dimensional vector W is (C-by-dim)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(b) # number of classes, C, each class has a bias \n",
        "    linear_part = jnp.dot(W, inputs) # (C-by-dim)*(dim-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(linear_part, axis=0) # largest of the class scores for each data point\n",
        "    bias_offset = jnp.dot(jnp.diag(b),jnp.ones((c, datalen))) # (C-by-C)*(C-by-L)\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(linear_part + bias_offset - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8847DKq3Jk9L"
      },
      "source": [
        "In what follows, the trick of setting the zeroth feature to be 1 is used to absorb the constant  $w_0$  into the dot product. Redefine the input data to be\n",
        "$x=(x1,…,xp)⟶x=(1,x1,…,xp)$.\n",
        " \n",
        "Correspondingly redefining the weight vectors to be  $w=(w0,w1,…,wp)$ , we have:\n",
        "$w_{k0}+w^⊤_kx⟶w^⊤_kx$.\n",
        " \n",
        "Thus the softmax_prob below has all the weights packaged into a matrix W as in the lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLRkL-6JCUd"
      },
      "source": [
        "def softmax_prob(W, inputs):  \n",
        "    # output is datalen-by-C\n",
        "    # inputs is (dim)-by-datalen\n",
        "    # W is C-by-(dim+1)\n",
        "    # Make sure all numerical operations are from JAX, so 'jnp', not 'np'\n",
        "    datalen = jnp.shape(inputs)[1] # how many points\n",
        "    c = len(W) # number of classes, C, each class has a bias\n",
        "    inputs = jnp.concatenate((jnp.ones((1,datalen)), inputs), axis=0)\n",
        "    # create inputs (dim+1)-by-datalen \n",
        "    score = jnp.dot(W,inputs) \n",
        "    # (C-by-(1+dim))*((1+dim)-by-datalen) = C-by-datalen\n",
        "    large = jnp.max(score, axis=0) # largest of the class scores for each data point\n",
        "    # subtract off the largest score from the bias of each class for stability to underflow/overflow\n",
        "    large_offset = jnp.dot(np.ones((c, datalen)),jnp.diag(large)) #  (C-by-L)*(L-by-L)    \n",
        "    expscore = jnp.exp(score  - large_offset)\n",
        "    norm_factor = jnp.diag(1/jnp.sum(expscore, axis=0))\n",
        "    return jnp.dot(expscore, norm_factor).T  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZmXBwhtJoYp"
      },
      "source": [
        "def softmax_xentropy(Wb, inputs, targets, num_classes):\n",
        "    epsilon = 1e-8\n",
        "    ys = get_one_hot(targets, num_classes)\n",
        "    logprobs = -jnp.log(softmax_prob(Wb, inputs)+epsilon)\n",
        "    return jnp.mean(ys*logprobs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tU86xW-JpYG"
      },
      "source": [
        "def get_one_hot(targets, num_classes):\n",
        "    res = jnp.eye(num_classes)[jnp.array(targets).reshape(-1)]\n",
        "    return res.reshape(list(targets.shape)+[num_classes])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVyU2kr7JqYJ"
      },
      "source": [
        "Wb = jnp.array([[-3., 1.3, 2.0, -1.0], [-6., -2., -3., 1.5], [1., 2.0, 2.0, 2.5], [3., 4.0, 4.0, -2.5]])\n",
        "# Build a toy dataset: 6 3-dim points with C=4  targets dim-by-datalen\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [3.82, -6.11, 3.15],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39],\n",
        "                   [0.14, -0.43, -1.69]]).T # transpose to make it a dim-by-datalen array\n",
        "targets = jnp.array([0, 1, 3, 2, 1, 2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xoxpsuGJsBP"
      },
      "source": [
        "# Initialize random model coefficients\n",
        "key = random.PRNGKey(0)\n",
        "key, W_key= random.split(key, 2)\n",
        "[classes, dim] = 4, 3\n",
        "Winit = random.normal(W_key, (classes, dim+1))\n",
        "print(Winit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY8vxPe3JvVI"
      },
      "source": [
        "## Automatic Differentiation used here\n",
        "Here, we will not explicitly define what the exact form of the gradient of the cross entropy loss function is. Recall, for linear regression, we computed the gradient and used it to reduce the loss. In this next code block, we will invoke\n",
        "\n",
        "grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "\n",
        "where the (0) is shorthand for argnums=0 which indicates that we take the gradient with respect to the first (using python's indexing convention of starting from 0) of the arguments of softmax_entropy. How this is done will be explored in another lab sheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d97S7opPJtdL"
      },
      "source": [
        "def grad_descent(Wb, inputs, targets, num_classes,  lrate, nsteps):\n",
        "    W1 = Wb\n",
        "    Whist = [W1]\n",
        "    losshist = [softmax_xentropy(W1,inputs, targets, num_classes )]\n",
        "    eta = lrate # learning rate\n",
        "    for i in range(nsteps):        \n",
        "        gWb = grad(softmax_xentropy, (0))(W1, inputs, targets, num_classes)\n",
        "        W1 = W1 - eta*gWb\n",
        "        if (i%5 ==0):\n",
        "            Whist.append(W1)\n",
        "            losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))\n",
        "    Whist.append(W1)\n",
        "    losshist.append(softmax_xentropy(W1, inputs, targets, num_classes))    \n",
        "    return W1, Whist, losshist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyMN5NFiJz1y"
      },
      "source": [
        "W2, Whist, losshist = grad_descent(Winit, inputs, targets, 4, 0.75, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB4ytRwaJ7hW"
      },
      "source": [
        "### Loss history\n",
        "Now that we have the initial weights Winit, the history of weights and the history of losses, we can see how the loss function reduces as a function of iteration step. You should experiment with different learning rates and iteration steps, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtwDXHjOJ8dm"
      },
      "source": [
        "plt.plot([5*i for i in range(len(losshist))], losshist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ntpzf30KFv8"
      },
      "source": [
        "Compare the predictions with the targets. First, we see what the randomly initialised weights produced as the predicted probabilities. Then we note the final (at the point that we stopped the iterations) prediction and compare that with the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MYK_0INJ3ef"
      },
      "source": [
        "print('From:\\n',np.around(softmax_prob(Winit, inputs),3))\n",
        "print('To:\\n',np.around(softmax_prob(W2, inputs),3))\n",
        "print('Target:\\n',get_one_hot(targets, 4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Bb0ZCEKL4_"
      },
      "source": [
        "# Your turn:\n",
        "Create your own input data and targets. You may choose them to be random. For instance, in numpy np.random.normal(mean, std_dev,(dim, datalen)) will create a set of datalen inputs of dimension dim, drawn from a normal distribution of a chosen mean and standard deviation. You must generate the Winit from jax.numpy in order to be able to use the gradient. Experiment with different learning rates, and see what you find."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1b1Lx0kKMPk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}