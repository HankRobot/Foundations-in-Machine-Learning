{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Perceptron.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMLlYFqwH76Ek+vq8Hxi7jI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HankRobot/Foundations-in-Machine-Learning/blob/main/The_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJFUy5hAqeIF"
      },
      "source": [
        "# **COMP3223 Machine Learning: Lab Sheet - The Perceptron**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F9LEfiAqkuc"
      },
      "source": [
        "# **Acknowledgements**\r\n",
        "\r\n",
        "This lab is inspired by the following blog post:\r\n",
        "\r\n",
        "(1) https://medium.com/@thomascountz/19-line-line-by-line-python-perceptron-b6f113b161f3\r\n",
        "\r\n",
        "# **Introduction**\r\n",
        "In the Foundations of Machine Learning module, you learnt about the perceptron, the neuron, and a little bit about the multilayer perceptron (MLP) so far. This lab will remind you of some of those ideas. In the lab, you will implement the perceptron algorithm. You will then test your implementation on various logical expressions to see the limits of the algorithm.\r\n",
        "\r\n",
        "Through this lab you'll learn how to:\r\n",
        "\r\n",
        "create and manipulate a perceptron to solve various logical expressions\r\n",
        "understand the limits of a perceptron\r\n",
        "To work through this lab you'll use the Python 3 language in a Jupyter Notebook environment, with the numpy package.\r\n",
        "\r\n",
        "# **Part 1: Implementing a Perceptron**\r\n",
        "The first part of this lab is to implement the perceptron learning algorithm to classify data for the logical 'OR function'.\r\n",
        "\r\n",
        "In the first lecture, we learnt about the perceptron. We discussed the formula that describes the process to perform a binary classification of inputs. We learnt that the perceptron takes in an input vector,  $x$ , multiplies it by a corresponding weight vector $w$ , and then adds it to a bias,  $b$ . It then uses an activation function, (the step function, for the case of the perceptron), to determine if our resulting summation is greater than  $0$ , in order to classify the output as  $1$ , or less than or equal to  $0$ , in order to classify the output as  $0$ . Note, you can assume that  $b=w_o$  and  $x_0=1$ . This results in the following expression:\r\n",
        "\r\n",
        "$y_j=f(∑^m_{i=o}w_ix_{ij})=1$ if $∑^m_{i=o}w_i⋅x_{ij}>0$\r\n",
        "\r\n",
        "$y_j=0$  otherwise\r\n",
        "\r\n",
        "Next, we learnt that by using labeled data, we could have our perceptron predict an output, determine if it was correct or not, and then adjust the weights and bias accordingly. The update equation is given as follows:\r\n",
        "\r\n",
        "$w_i←w_i−η∗(y_j−t_j)∗x_{ij}$ \r\n",
        "Where  $η$  represents the learning rate. Start by filling in the missing code for the perceptron class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ebqlUfOqzXH"
      },
      "source": [
        "class Perceptron(object):\r\n",
        "\r\n",
        "    def __init__(self, no_of_inputs, threshold=100, learning_rate=0.01):\r\n",
        "        self.threshold = threshold\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.weights = np.ones(no_of_inputs + 1)\r\n",
        "           \r\n",
        "    def predict(self, inputs):\r\n",
        "        # write the code to implement the activation output (i.e. y_j from the expression above) \r\n",
        "        ## code goes here\r\n",
        "        return activation\r\n",
        "\r\n",
        "    def train(self, training_inputs, labels):\r\n",
        "        for _ in range(self.threshold):\r\n",
        "            for inputs, label in zip(training_inputs, labels):\r\n",
        "                prediction = self.predict(inputs)\r\n",
        "                # write the code to compute the weight updates\r\n",
        "                ## code goes here"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "PeINUJV5qacu",
        "outputId": "79af6ea1-02fb-40e0-eaf9-dc87e24bec94"
      },
      "source": [
        "# The following code can be used to test your Perceptron\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "training_inputs = []\r\n",
        "training_inputs.append(np.array([0, 0]))\r\n",
        "training_inputs.append(np.array([1, 0]))\r\n",
        "training_inputs.append(np.array([0, 1]))\r\n",
        "training_inputs.append(np.array([1, 1]))\r\n",
        "\r\n",
        "labels = np.array([0, 1, 1, 1])\r\n",
        "\r\n",
        "perceptron = Perceptron(2)\r\n",
        "perceptron.train(training_inputs, labels)\r\n",
        "\r\n",
        "inputs = np.array([1, 1])\r\n",
        "perceptron.predict(inputs) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a11e921b2f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mperceptron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-37ae452c2306>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_inputs, labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;31m# write the code to compute the weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;31m## code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-37ae452c2306>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# write the code to implement the activation output (i.e. y_j from the expression above)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m## code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'activation' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS9xITPe-QPY"
      },
      "source": [
        "Once your perceptron is working, try varying the learning rate.\r\n",
        "\r\n",
        "After how many iterations does your perceptron converge?\r\n",
        "\r\n",
        "Does your perceptron correctly classify the OR function?\r\n",
        "\r\n",
        "Does your perceptron correctly classify the XOR function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LjnY6SI-XEB"
      },
      "source": [
        "# **Part 2:**\r\n",
        "Implementing a Multilayer Perceptron with a Single Hidden Layer\r\n",
        "We can think of an MLP as extending the Perceptron by making the following changes:\r\n",
        "\r\n",
        "adding at least one additional layer to the network which can have an arbitrary number of neurons. For simplicity you can assume the hidden layer has 2 neurons + 1 bias.\r\n",
        "modifying the activation function of the neurons from the step function to the sigmoid function. Note, a function defining the sigmoid is given below.\r\n",
        "incorporating the backpropagation algorithm for learning as follows\r\n",
        "The forward pass of the algorithm simply computes what the output of the MLP will generate following the equation we covered in the lecture:\r\n",
        "\r\n",
        "# **FORWARD PASS**\r\n",
        "$y=f(∑^{nH}_{j=1}w_j⋅f(∑^d_{i=1}w_{ji}x_i+w_{j0})+w_0))$ \r\n",
        "Note, here the biases are written as  wj0  and  w0  for consistency with the lecture slides (where k=1, we only have one output neuron for simplicity). For this lab you can assume that  f  in both cases is the sigmoid function, defined by the function named sigmoid in the code below.\r\n",
        "\r\n",
        "# **BACKWARD PASS**\r\n",
        "Then, to update the weights, we need to compute the loss. Let's assume the following expression:\r\n",
        "\r\n",
        "$L=0.5∑_j{(y_j−t_j)^2}$ \r\n",
        "Note, the  0.5  term just allows the expression to be simplified later on as we need to compute the derivative of the loss to update the weights.\r\n",
        "\r\n",
        "$w←w−η∗Δw$ \r\n",
        "where the update of the weights between the hidden layer and the output layer differs from the update of the weights between the hidden layer and input layer. You can differentiate the total loss  L  with respect to the weight  w  you are updating to find  Δw .\r\n",
        "\r\n",
        "You can work with the snippets of code given below. However, if you prefer to rewrite the code yourself that is great.\r\n",
        "\r\n",
        "For more details about the derivation of  Δw  see this page: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\r\n",
        "\r\n",
        "Note, another good source for the backpropagation algorithm can be found here: http://neuralnetworksanddeeplearning.com/chap1.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOJv6v3G-bNM"
      },
      "source": [
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "    print(x)\r\n",
        "    return 1 / (1 + math.exp(-x))\r\n",
        "\r\n",
        "class MultiLayerPerceptron(object):\r\n",
        "    def __init__(self,dim_input, dim_output):\r\n",
        "        self.weights1   = np.random.rand(dim_input,3)\r\n",
        "        self.weights2   = np.random.rand(3,1)                 \r\n",
        "        self.output     = np.zeros(dim_output)\r\n",
        "\r\n",
        "    def feedforward(self, inputs):\r\n",
        "        ## implement code here \r\n",
        "\r\n",
        "    def backprop(self, inputs, label):\r\n",
        "        ## implement code here\r\n",
        "\r\n",
        "    def train(self,iterations, x, y):\r\n",
        "        for _ in range(iterations):\r\n",
        "            ## implement code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZANHhKf-Ve7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTcUla5ir9wu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}